<br>
<br>
<center> <font size = 5> å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ </font></center>
<br>
<br>
<center> <font size = 6> å®éªŒæŠ¥å‘Š </font></center>
<br>
<br>
<br>
<center> <font size = 4> 
è¯¾ç¨‹åç§°ï¼šæœºå™¨å­¦ä¹  <br/>
è¯¾ç¨‹ç±»å‹ï¼šå¿…ä¿®  <br/>
å®éªŒé¢˜ç›®ï¼šå¤šé¡¹å¼æ‹Ÿåˆæ­£å¼¦å‡½æ•° 
</font></center>
<br>
<br>
<center> <font size = 4> å­¦å·ï¼š1181000420 </font></center>
<center> <font size = 4> å§“åï¼šéŸ¦æ˜†æ° </font></center>

<div STYLE="page-break-after: always;"></div>
<!-- æ­¤å¤„ç”¨äºæ¢è¡Œ -->

<font size = 5> ä¸€ã€å®éªŒç›®çš„ </font>

æŒæ¡æœ€å°äºŒä¹˜æ³•æ±‚è§£ï¼ˆæ— æƒ©ç½šé¡¹çš„æŸå¤±å‡½æ•°ï¼‰ã€æŒæ¡åŠ æƒ©ç½šé¡¹ï¼ˆ2èŒƒæ•°ï¼‰çš„æŸå¤±å‡½æ•°ä¼˜åŒ–ã€æ¢¯åº¦ä¸‹é™æ³•ã€å…±è½­æ¢¯åº¦æ³•ã€ç†è§£è¿‡æ‹Ÿåˆã€å…‹æœè¿‡æ‹Ÿåˆçš„æ–¹æ³•(å¦‚åŠ æƒ©ç½šé¡¹ã€å¢åŠ æ ·æœ¬)

<font size = 5> äºŒã€å®éªŒè¦æ±‚åŠå®éªŒç¯å¢ƒ  </font>



<font size = 4> å®éªŒè¦æ±‚ </font>

1. ç”Ÿæˆæ•°æ®ï¼ŒåŠ å…¥å™ªå£°ï¼›
2. ç”¨é«˜é˜¶å¤šé¡¹å¼å‡½æ•°æ‹Ÿåˆæ›²çº¿ï¼›
3. ç”¨è§£æè§£æ±‚è§£ä¸¤ç§lossçš„æœ€ä¼˜è§£ï¼ˆæ— æ­£åˆ™é¡¹å’Œæœ‰æ­£åˆ™é¡¹ï¼‰
4. ä¼˜åŒ–æ–¹æ³•æ±‚è§£æœ€ä¼˜è§£ï¼ˆæ¢¯åº¦ä¸‹é™ï¼Œå…±è½­æ¢¯åº¦ï¼‰ï¼›
5. ç”¨ä½ å¾—åˆ°çš„å®éªŒæ•°æ®ï¼Œè§£é‡Šè¿‡æ‹Ÿåˆã€‚
6. ç”¨ä¸åŒæ•°æ®é‡ï¼Œä¸åŒè¶…å‚æ•°ï¼Œä¸åŒçš„å¤šé¡¹å¼é˜¶æ•°ï¼Œæ¯”è¾ƒå®éªŒæ•ˆæœã€‚
7. è¯­è¨€ä¸é™ï¼Œå¯ä»¥ç”¨matlabï¼Œpythonã€‚æ±‚è§£è§£æè§£æ—¶å¯ä»¥åˆ©ç”¨ç°æˆçš„çŸ©é˜µæ±‚é€†ã€‚æ¢¯åº¦ä¸‹é™ï¼Œå…±è½­æ¢¯åº¦è¦æ±‚è‡ªå·±æ±‚æ¢¯åº¦ï¼Œè¿­ä»£ä¼˜åŒ–è‡ªå·±å†™ã€‚ä¸è®¸ç”¨ç°æˆçš„å¹³å°ï¼Œä¾‹å¦‚pytorchï¼Œtensorflowçš„è‡ªåŠ¨å¾®åˆ†å·¥å…·ã€‚

<font size = 4> å®éªŒç¯å¢ƒ  </font>

- Windows10  
- PyCharm   
- VSCode

<font size = 5> ä¸‰ã€è®¾è®¡æ€æƒ³(æœ¬ç¨‹åºä¸­çš„ç”¨åˆ°çš„ä¸»è¦ç®—æ³•åŠæ•°æ®ç»“æ„) </font>


<font size = 4> 1.ç®—æ³•åŸç† </font>


æˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å‡½æ•°y(x,w)æ¥æ¨¡æ‹Ÿæ­£å¼¦å‡½æ•°æ›²çº¿
$$y(x,w)=w_{0}+w_{1}x+\cdots+w_{m}x^{m}=\sum_{i=0}^{m}w_{i}x^{i}\tag{1}$$
ä¸‹é¢æ˜¯ï¼šå»ºç«‹è¯¯å·®å‡½æ•°ï¼Œæµ‹é‡æ¯ä¸ªæ ·æœ¬ç‚¹ç›®æ ‡å€¼tä¸é¢„æµ‹å‡½æ•°yä¹‹é—´çš„è¯¯å·®
$$E(w)=\frac{1}{2}\sum_{n=1}^{N}\lbrace y(x_{n},w)-t_{n} \rbrace\ ^{2} \tag{2}$$
ç„¶åå¯ä»¥ä»¤
$X=\begin{bmatrix}
    1&x_{1}&\cdots&x_{1}^{m}\\
    1&x_{2}&\cdots&x_{2}^{m}\\
    \vdots&\vdots&\ddots&\vdots\\
    1&x_{N}&\cdots&x_{N}^{m}
\end{bmatrix}$  $W=\begin{bmatrix}
    w_{0}\\w{1}\\\vdots\\w_{m}
\end{bmatrix}$   $T=\begin{bmatrix}
    t_{1}\\t_{2}\\\vdots\\t_{N}
\end{bmatrix}$


å°†(2)å¼åŒ–æˆçŸ©é˜µçš„å½¢å¼å¦‚ä¸‹ï¼š
$$E(w)=\frac{1}{2}(XW-T)'(XW-T)\tag{3}$$


$E$æ˜¯$w$çš„äºŒæ¬¡å‡½æ•°ï¼Œå¯¹å…¶æ±‚åå¯¼,
ç»“æœå¦‚ä¸‹ï¼š
    $$\frac{\partial E} {\partial w} = X'(XW-T)\tag{4}$$

ç„¶åè®¾å¯¼æ•°ä¸º0ï¼Œå­˜åœ¨å”¯ä¸€è§£$w^{*}$
$$w^{*}=(X'X)^{-1}X'T\tag{5}$$
\
\
ä¸‹é¢æˆ‘ä»¬åœ¨ä¼˜åŒ–ç›®æ ‡å‡½æ•°$E(w)$ä¸­åŠ å…¥å¯¹$w$çš„æƒ©ç½š

$$\widetilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}\lbrace y(x_{n},w)-t_{n} \rbrace ^{2}+\frac{\lambda}{2}||w^{2}||\tag{6}$$

å°†(6)å¼åŒ–æˆçŸ©é˜µçš„å½¢å¼å¦‚ä¸‹ï¼š
$$\widetilde{E}(w)=\frac{1}{2}(XW-T)'(XW-T)+\frac{\lambda}{2}W'W\tag{7}$$
$\widetilde{E}$æ˜¯$w$çš„äºŒæ¬¡å‡½æ•°ï¼Œå¯¹å…¶æ±‚åå¯¼,
ç»“æœå¦‚ä¸‹ï¼š
$$\frac{\partial \widetilde{E}}{\partial w}=X'XW-X'T+\lambda W\tag{8}$$
ç„¶åè®¾å¯¼æ•°ä¸º0ï¼Œå­˜åœ¨å”¯ä¸€è§£$w^{*}$
$$w^{*}=(X'X+\lambda I)^{-1}X'T\tag{9}$$
å…¶ä¸­$I$ä¸ºå•ä½çŸ©é˜µ




<font size = 4> 2.ç®—æ³•çš„å®ç°  </font>

- è§£æè§£æ³•(æ— æ­£åˆ™é¡¹)
ç”±(5)å¼å¯ä»¥é€šè¿‡Xå’ŒTæ±‚å‡ºW*ï¼Œä¸‹é¢æ˜¯å…·ä½“å®ç°çš„Pythonå‡½æ•°ä»£ç :
```python
     def fit_without_regulation(self, X, T):
        """
        è®¡ç®—æœªåŠ å…¥æ­£åˆ™é¡¹çš„æ•°å€¼è§£W
        param X: äºŒç»´array X
        param T: array T
        return: æ¨¡å‹å‚æ•°W
        """
        return np.linalg.pinv(X) @ T
```
- è§£æè§£æ³•(æœ‰æ­£åˆ™é¡¹)
ç”±(9)å¼å¯ä»¥é€šè¿‡Xå’ŒTæ±‚å‡ºW*ï¼Œä¸‹é¢æ˜¯å…·ä½“å®ç°çš„Pythonå‡½æ•°ä»£ç :
```python
    def fit_with_regulation(self, X, T, Lambda=1e-7):
        """
        è®¡ç®—åŠ å…¥æ­£åˆ™é¡¹çš„æ•°å€¼è§£W
        param X: äºŒç»´array X
        param T: array T
        param Lambda:æƒé‡å‚æ•°
        return: æ¨¡å‹å‚æ•°W
        """
        return np.linalg.solve(X.T @ X + Lambda * np.identity(self.m + 1), X.T @ T)
```

- æ¢¯åº¦ä¸‹é™æ³•
æ¢¯åº¦ä¸‹é™æ³•(gradient descent)æ˜¯æœ€ä½³åŒ–ç†è®ºé‡Œé¢çš„ä¸€ä¸ªä¸€é˜¶æ‰¾æœ€ä½³è§£çš„ä¸€ç§æ–¹æ³•,å¿…é¡»å‘å‡½æ•°ä¸Šå½“å‰ç‚¹å¯¹åº”æ¢¯åº¦ï¼ˆæˆ–è€…æ˜¯è¿‘ä¼¼æ¢¯åº¦ï¼‰çš„åæ–¹å‘çš„è§„å®šæ­¥é•¿è·ç¦»ç‚¹è¿›è¡Œè¿­ä»£æœç´¢.
åœ¨æœ¬å®éªŒä¸­ï¼Œæ¢¯åº¦ä¸ºW*ï¼Œä¸Šé¢çš„å…¬å¼(4)å³ä¸ºæ— æ­£åˆ™é¡¹çš„æ¢¯åº¦ï¼š
 $$\frac{\partial E} {\partial w} = X'(XW-T)\tag{4}$$
è€Œä¸Šé¢çš„å…¬å¼(8)å³ä¸ºæœ‰æ­£åˆ™é¡¹çš„æ¢¯åº¦ï¼š
$$\frac{\partial E}{\partial w}=X'XW-X'T+\lambda W\tag{8}$$
å› æ­¤æ¯æ¬¡æ›´æ–°Wå…¬å¼å¦‚ä¸‹ï¼š
$$W=W-learning\_rate*\frac{\partial E}{\partial w}$$
ä¸‹é¢æ˜¯å…·ä½“å®ç°çš„Pythonå‡½æ•°ä»£ç :
```python
    def gradient_descent(self, X, T, rate=0.1, precision=1e-1):
        """
        é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è®¡ç®—ç»™å®šlearning rateå’Œç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°
        param X: äºŒç»´array X
        param T: array T
        param rate: learning rate
        return: è®¡ç®—å‡ºè¦æ±‚ç²¾åº¦æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°
        """
        times = 0
        W = np.zeros((self.m + 1, 1))  # åˆå§‹åŒ–Wä¸ºå…¨ä¸º0çš„åˆ—å‘é‡
        while self.error(X, W, T) > precision:
            last_error = self.error(X, W, T)
            W = W - rate * X.T @ (X @ W - T)
            if self.error(X, W, T) > last_error:
                rate = rate / 2
            times += 1
        return times
```

- å…±è½­æ¢¯åº¦æ³•
ä¸‹é¢æ˜¯wikipediaç»™å‡ºçš„ç®—æ³•å®ç°ï¼š
<br>
![avatar](https://wikimedia.org/api/rest_v1/media/math/render/svg/021e02360a28c46188bc915eb06533dfa84a3002)
ä¸‹é¢æ˜¯å…·ä½“å®ç°çš„Pythonå‡½æ•°ä»£ç :
```python
 def conjugate_gradient(self, X, T, precision=1e-5, times=0,Lambda=0):
        """
        é€šè¿‡å…±è½­æ¢¯åº¦æ³•è®¡ç®—ç»™å®šç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°,
        å‚è€ƒ http://en.wikipedia.org/wiki/Conjugate_gradient_method

        param X: äºŒç»´array X
        param T: array T
        param precision:ç²¾åº¦è¦æ±‚ï¼Œé»˜è®¤ä¸º0.1
        param times: è¿­ä»£æ¬¡æ•°
        param Lambda:æƒ©ç½šé¡¹å‚æ•°ï¼Œé»˜è®¤ä¸º0ï¼Œå³ä¸ºæ— æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        return: è¦æ±‚ç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°å’ŒW
        """

        # è½¬åŒ–æˆAx = bçš„å½¢å¼æ±‚è§£
        A = X.T @ X + Lambda * np.identity(self.m+1)        #Lambda=0æ—¶ä¸ºæ— æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        x = np.zeros((self.m + 1, 1))                       #Lambda>0æ—¶ä¸ºæœ‰æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        b = X.T @ T

        r = b - np.dot(A, x)
        p = r
        rsold = r.T @ r
        # è®¡ç®—ç»™å®šè¿­ä»£æ¬¡æ•°å¾—åˆ°çš„W
        if times != 0:
            for i in range(times):
                Ap = A @ p
                alpha = rsold[0][0] / np.dot(p.T, Ap)[0][0]
                x = x + alpha * p
                r = r - alpha * Ap
                rsnew = r.T @ r
                p = r + (rsnew / rsold) * p
                rsold = rsnew
            return x
        times = 0
        while self.error(X, x, T) > precision:
            Ap = A @ p
            alpha = rsold[0][0] / np.dot(p.T, Ap)[0][0]
            x = x + alpha * p
            r = r - alpha * Ap
            rsnew = r.T @ r
            p = r + (rsnew / rsold) * p
            rsold = rsnew
            times += 1
        return times, x
```
<font size = 5> å››ã€å®éªŒç»“æœåˆ†æ </font>

<font size = 4>1.ä¸å¸¦æƒ©ç½šé¡¹çš„è§£æè§£ </font>

å…ˆåˆ©ç”¨$sin(2\pi x)$äº§ç”Ÿæ ·æœ¬,æ ·æœ¬æ•°æ®é‡$size=10$ï¼Œ$x$å‡åŒ€åˆ†å¸ƒåœ¨$[0,1]$,ç„¶åå¯¹æ¯ä¸ªç›®æ ‡å€¼tåŠ ä¸€ä¸ª0å‡å€¼çš„é«˜æ–¯å™ªå£°ï¼Œè¿™æ ·å°±äº§ç”Ÿäº†æ ·æœ¬æ•°æ®:

![myplot1](https://i.imgur.com/jMmvEjR.png)


åœ¨ç”¨äºæ‹Ÿåˆçš„å¤šé¡¹å¼å‡½æ•°é˜¶æ•°å–ä¸åŒå€¼æ—¶ï¼Œè§‚å¯Ÿæœ€ä½³æ‹Ÿåˆæ›²çº¿æƒ…å†µï¼Œå¦‚ä¸‹å›¾æ˜¯Måˆ†åˆ«å–0-9çš„æ›²çº¿æ‹Ÿåˆå›¾(æœªåŠ æƒ©ç½šé¡¹)ï¼š


<img src = "https://i.imgur.com/IyY3daS.png" width="49%">
<img src = "https://i.imgur.com/L1EEq7h.png" width="49%">
<img src = "https://i.imgur.com/yvYv69Y.png" width="49%">
<img src = "https://i.imgur.com/nAvMgoR.png" width="49%">

åŒæ—¶æˆ‘ä»¬æ¯”è¾ƒè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„æ–¹å‡æ ¹å€¼$E_{RMS}$éšMçš„å˜åŒ–æƒ…å†µï¼Œå¦‚ä¸‹å›¾ï¼š
![myplot](https://i.imgur.com/VidDR2P.png)

æˆ‘ä»¬å¯ä»¥å‘ç°

- å½“Mè¾ƒå°æ—¶ï¼Œéšç€Mçš„å¢å¤§ï¼Œè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„è¯¯å·®éƒ½åœ¨é™ä½ï¼Œä½†å½“Må¢å¤§åˆ°æŸä¸€æ•°å€¼åï¼Œè®­ç»ƒæ•°æ®çš„æ–¹å‡æ ¹å€¼$E_{RMS}$ä»åœ¨é™ä½ï¼Œè€Œæµ‹è¯•æ•°æ®çš„æ–¹å‡æ ¹å€¼$E_{RMS}$å´åœ¨å˜å¤§ï¼Œè¯´æ˜å‡ºç°äº†è¿‡æ‹Ÿåˆ


ä¸‹é¢æˆ‘ä»¬æ¥åˆ†æè®­ç»ƒæ ·æœ¬æ•°é‡å¯¹æ‹Ÿåˆçš„ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜æ ·æœ¬æ•°é‡ï¼Œæ¥è§‚å¯Ÿæ›²çº¿çš„æ‹Ÿåˆæƒ…å†µï¼Œä¸‹é¢æ˜¯N=15,M=9å’ŒN=100ï¼ŒM=9çš„æ¯”è¾ƒï¼š
<img src = "https://i.imgur.com/sUwTa2E.png" width="49%">
<img src = "https://i.imgur.com/krPGlc0.png" width="49%">
- æˆ‘ä»¬å‘ç°å¢å¤§æ ·æœ¬æ•°é‡å¯å‡å°‘è¿‡å­¦ä¹ ç¨‹åº¦
<br>
<br>

<font size = 4>2.å¸¦æƒ©ç½šé¡¹çš„è§£æè§£ </font>

å¯ä»¥åœ¨ä¼˜åŒ–ç›®æ ‡å‡½æ•°ğ¸(ğ‘¤)ä¸­åŠ å…¥å¯¹ğ‘¤çš„æƒ©ç½šä½œä¸ºæ­£åˆ™é¡¹ï¼Œå¯¹äºæƒé‡å‚æ•°$\lambda$çš„é€‰æ‹©ï¼Œæˆ‘ä»¬åˆ†ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ç”»å‡º$E_{RMS}$éš$\lambda$çš„å˜åŒ–æƒ…å†µ(é˜¶æ•°=5ï¼Œæ•°æ®é‡=10)ï¼Œç»“æœå‘ç°$\lambda=10^{-7}$æ—¶æ•ˆæœæœ€å¥½,å¦‚ä¸‹å›¾ï¼š

![myplot2](https://i.imgur.com/o6aTFse.png)


å› æ­¤æˆ‘ä»¬è®¾ç½®æƒé‡å‚æ•°$\lambda$ä¸º$10^{-7}$ï¼Œåœ¨ç”¨äºæ‹Ÿåˆçš„å¤šé¡¹å¼å‡½æ•°é˜¶æ•°å–ä¸åŒå€¼æ—¶ï¼Œè§‚å¯Ÿæœ€ä½³æ‹Ÿåˆæ›²çº¿æƒ…å†µï¼Œå¦‚ä¸‹æ˜¯size=10,måˆ†åˆ«ç­‰äº0ï¼Œ5ï¼Œ9æ—¶çš„æ›²çº¿æ‹Ÿåˆå›¾(æœ‰æƒ©ç½šé¡¹ä¸æ— æƒ©ç½šé¡¹å¯¹æ¯”):

<img src = "https://i.imgur.com/D5xoC7V.png" width="49%">
<img src = "https://i.imgur.com/ZfGss4z.png" width="49%">
<img src = "https://i.imgur.com/TimnxtQ.png" width="49%">
<img src = "https://i.imgur.com/wiJcJ6S.png" width="49%">
<img src = "https://i.imgur.com/9qk044V.png" width="49%">
<img src = "https://i.imgur.com/V2lavCz.png" width="49%">


<br>
<br>


<font size = 4>3.æ¢¯åº¦ä¸‹é™æ³•å’Œå…±è½­æ¢¯åº¦æ³•æ‹Ÿåˆæƒ…å†µæ¯”è¾ƒ </font>

ä¸Šé¢æˆ‘ä»¬æ¯”è¾ƒäº†åœ¨è§£æè§£ä¸‹æœ‰æ— æƒ©ç½šé¡¹æ‹Ÿåˆçš„æ¯”è¾ƒï¼Œä¸‹é¢æˆ‘ä»¬æ¯”è¾ƒæ¢¯åº¦ä¸‹é™æ³•å’Œå…±è½­æ¢¯åº¦æ³•å¯¹æœ‰æ— æƒ©ç½šé¡¹çš„æ‹Ÿåˆæƒ…å†µ(è¿­ä»£æ¬¡æ•°ä¸º100æ¬¡)ï¼š

<img src = "https://i.imgur.com/hpoUvwT.png" width="49%">
<img src = "https://i.imgur.com/NB1SLjR.png" width="49%">
<img src = "https://i.imgur.com/uWU6K9q.png" width="49%">
<img src = "https://i.imgur.com/WgneeZL.png" width="49%">
<img src = "https://i.imgur.com/fuuls4J.png" width="49%">
<img src = "https://i.imgur.com/zOmbxDj.png" width="49%">
<img src = "https://i.imgur.com/iy6fGuF.png" width="49%">
<img src = "https://i.imgur.com/AIvZmZ1.png" width="49%">

- ä»ä¸Šé¢çš„å¯¹æ¯”å¯ä»¥å‘ç°ï¼Œç›¸åŒçš„è¿­ä»£æ¬¡æ•°ï¼Œå…±è½­æ¢¯åº¦æ¯”æ¢¯åº¦ä¸‹é™çš„æ‹Ÿåˆæƒ…å†µå¥½å¾—å¤š

ä¸‹é¢æˆ‘ä»¬å¯¹ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•å’Œå…±è½­æ¢¯åº¦æ³•è¾¾åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€æ¬¡æ•°çš„ç®€å•æ¯”è¾ƒï¼Œå‘ç°ï¼š
```
ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-1æ‰€éœ€æ¬¡æ•°:38305
ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-1æ‰€éœ€æ¬¡æ•°:5
ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-5æ‰€éœ€æ¬¡æ•°:161
```
- å…±è½­æ¢¯åº¦æ³•ç›¸è¾ƒæ¢¯åº¦ä¸‹é™æ³•ï¼Œå¯¹äºç›¸åŒè®­ç»ƒæ•°æ®è¾¾åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€æ¬¡æ•°å°å¾—å¤šï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«



<font size = 5> äº”ã€ç»“è®º </font>

- é˜¶æ•°Mè¾ƒå°æ—¶ï¼Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼ˆä¸çµæ´»ï¼‰ï¼Œæ–¹å‡æ ¹è¯¯å·®å¤§
- é˜¶æ•°M=3-8æ—¶ï¼Œæ‹Ÿåˆè¾ƒå¥½ï¼Œæ–¹å‡æ ¹è¯¯å·®è¾ƒå°
- é˜¶æ•°Mè¿‡å¤§æ—¶ï¼Œè™½ç„¶è¯¯å·®è¿›ä¸€æ­¥é™ä½ï¼Œä½†å‘ç”Ÿäº†è¿‡æ‹Ÿåˆï¼Œå¯¹æ–°çš„æµ‹è¯•æ•°æ®è¯¯å·®åè€Œå˜å¤§
- å¢å¤§æ ·æœ¬æ•°é‡å¯å‡å°‘è¿‡å­¦ä¹ ç¨‹åº¦
- åœ¨ä¼˜åŒ–ç›®æ ‡å‡½æ•°åŠ å…¥æ­£åˆ™é¡¹ä½œä¸ºæƒ©ç½šå¯ä»¥é¿å…$w^{*}$å…·æœ‰è¿‡å¤§çš„ç»å¯¹å€¼ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆ
- æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œæ‰€éœ€è¿­ä»£æ¬¡æ•°é«˜ï¼Œè€Œå…±è½­æ¢¯åº¦æ”¶æ•›é€Ÿåº¦å¿«ï¼Œæ‰€éœ€è¿­ä»£æ¬¡æ•°ä½ï¼Œå¯¹å¤šé¡¹å¼æ›²çº¿æ‹Ÿåˆçš„æ•ˆæœæ›´å¥½

<font size = 5> å…­ã€å‚è€ƒæ–‡çŒ® </font>
- [1] [gradient descent wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)
- [2] [conjugate gradient method wikipedia](https://en.wikipedia.org/wiki/Conjugate_gradient_method)
- [3] [æ·±å…¥æµ…å‡º--æ¢¯åº¦ä¸‹é™æ³•åŠå…¶å®ç°](https://www.jianshu.com/p/c7e642877b0e)
- [4] [æœºå™¨/æ·±åº¦å­¦ä¹ -åŸºç¡€æ•°å­¦(äºŒ):æ¢¯åº¦ä¸‹é™æ³•(gradient descent)](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)
  


<font size = 5> ä¸ƒã€é™„å½•:æºä»£ç (å¸¦æ³¨é‡Š) </font>
```python
import numpy as np
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore")


class lab1:
    def __init__(self, size, m):
        self.size = size  # æ ·æœ¬æ•°æ®é‡
        self.m = m  # æ‹Ÿåˆå¤šé¡¹å¼å‡½æ•°çš„é˜¶æ•°

    def generate_data(self):
        """
        äº§ç”Ÿsizeä¸ªæ•°æ®
        return:
        x:xè½´ç‚¹çš„array
        y:yè½´ç‚¹çš„arrayï¼Œy=sin(2*pi*x)
        """
        x = np.linspace(start=0, stop=1, num=self.size)
        mu, sigma = 0, 0.1  # mean and standard deviation
        y = np.sin(2 * np.pi * x) + np.random.normal(mu, sigma, size=self.size)
        return x, y

    def trans(self, x, y):
        """
        å°†x,yä¸¤ä¸ªarrayæ ¹æ®å¤šé¡¹å¼å‡½æ•°çš„é˜¶mè½¬åŒ–æˆç›¸åº”çš„array Xï¼ŒT
        param x: xè½´ç‚¹çš„array
        param y: yè½´ç‚¹çš„arrayï¼Œy=sin(2*pi*x)
        return:
        Xï¼ŒT:å¯¹åº”çš„ä¸¤ä¸ªarray
        """
        X = np.zeros((self.size, self.m + 1))
        for i in range(self.m + 1):
            for j in range(self.size):
                X[j, i] = np.power(x[j], i)
        T = y.reshape(self.size, 1)
        return X, T

    def error(self, X, W, T):
        """
        æ ¹æ®X,W,Tä¸‰ä¸ªçŸ©é˜µæ ¹æ®å…¬å¼è®¡ç®—è¯¯å·®
        param X: äºŒç»´array X
        param W: array W
        param T: array T
        return: è¯¯å·®å‡½æ•°çš„è®¡ç®—ç»“æœ
        """
        error = 1 / 2 * (X @ W - T).T @ (X @ W - T)  # error.shape = (1,1)
        return error[0][0]

    def E_RMS(self, error):
        """
        è®¡ç®—è¯¯å·®çš„æ–¹å‡æ ¹å€¼
        return: è¯¯å·®çš„çš„æ–¹å‡æ ¹å€¼E_RMS
        """
        return np.sqrt(2 / self.size * error)

    def fit_without_regulation(self, X, T):
        """
        è®¡ç®—æœªåŠ å…¥æ­£åˆ™é¡¹çš„æ•°å€¼è§£W
        param X: äºŒç»´array X
        param T: array T
        return: æ¨¡å‹å‚æ•°W
        """
        return np.linalg.pinv(X) @ T

    def fit_with_regulation(self, X, T, Lambda=1e-7):
        """
        è®¡ç®—åŠ å…¥æ­£åˆ™é¡¹çš„æ•°å€¼è§£W
        param X: äºŒç»´array X
        param T: array T
        param Lambda:æƒé‡å‚æ•°
        return: æ¨¡å‹å‚æ•°W
        """
        return np.linalg.solve(X.T @ X + Lambda * np.identity(self.m + 1), X.T @ T)

    def gradient_descent(self, X, T, rate=0.1, precision=1e-1, times=0,Lambda=0):
        """
        é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è®¡ç®—ç»™å®šlearning rateå’Œç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°
        param X: äºŒç»´array X
        param T: array T
        param rate: learning rate
        param precision:ç²¾åº¦è¦æ±‚ï¼Œé»˜è®¤ä¸º0.1
        param times: è¿­ä»£æ¬¡æ•°
        param Lambda:æƒ©ç½šé¡¹å‚æ•°ï¼Œé»˜è®¤ä¸º0ï¼Œå³ä¸ºæ— æ­£åˆ™é¡¹çš„æ¢¯åº¦ä¸‹é™
        return: è®¡ç®—å‡ºè¦æ±‚ç²¾åº¦æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°å’ŒW
        """
        W = np.zeros((self.m + 1, 1))  # åˆå§‹åŒ–Wä¸ºå…¨ä¸º0çš„åˆ—å‘é‡
        # è®¡ç®—ç»™å®šè¿­ä»£æ¬¡æ•°å¾—åˆ°çš„W
        if times != 0:
            for i in range(times):
                last_error = self.error(X, W, T)
                W = W - rate * (X.T @ (X @ W - T) + Lambda * W)     #Lambda=0æ—¶ä¸ºæ— æ­£åˆ™é¡¹çš„æ¢¯åº¦ä¸‹é™
                if self.error(X, W, T) > last_error:                #Lambda>0æ—¶ä¸ºæœ‰æ­£åˆ™é¡¹çš„æ¢¯åº¦ä¸‹é™
                    rate = rate / 2
            return W
        times = 0
        while self.error(X, W, T) > precision:
            last_error = self.error(X, W, T)
            W = W - rate * (X.T @ (X @ W - T) + Lambda * W)
            if self.error(X, W, T) > last_error:
                rate = rate / 2
            times += 1
        return times, W

    def conjugate_gradient(self, X, T, precision=1e-5, times=0,Lambda=0):
        """
        é€šè¿‡å…±è½­æ¢¯åº¦æ³•è®¡ç®—ç»™å®šç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°,
        å‚è€ƒ http://en.wikipedia.org/wiki/Conjugate_gradient_method

        param X: äºŒç»´array X
        param T: array T
        param precision:ç²¾åº¦è¦æ±‚ï¼Œé»˜è®¤ä¸º0.1
        param times: è¿­ä»£æ¬¡æ•°
        param Lambda:æƒ©ç½šé¡¹å‚æ•°ï¼Œé»˜è®¤ä¸º0ï¼Œå³ä¸ºæ— æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        return: è¦æ±‚ç²¾åº¦è¦æ±‚æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°å’ŒW
        """

        # è½¬åŒ–æˆAx = bçš„å½¢å¼æ±‚è§£
        A = X.T @ X + Lambda * np.identity(self.m+1)        #Lambda=0æ—¶ä¸ºæ— æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        x = np.zeros((self.m + 1, 1))                       #Lambda>0æ—¶ä¸ºæœ‰æ­£åˆ™é¡¹çš„å…±è½­æ¢¯åº¦
        b = X.T @ T

        r = b - np.dot(A, x)
        p = r
        rsold = r.T @ r
        # è®¡ç®—ç»™å®šè¿­ä»£æ¬¡æ•°å¾—åˆ°çš„W
        if times != 0:
            for i in range(times):
                Ap = A @ p
                alpha = rsold[0][0] / np.dot(p.T, Ap)[0][0]
                x = x + alpha * p
                r = r - alpha * Ap
                rsnew = r.T @ r
                p = r + (rsnew / rsold) * p
                rsold = rsnew
            return x
        times = 0
        while self.error(X, x, T) > precision:
            Ap = A @ p
            alpha = rsold[0][0] / np.dot(p.T, Ap)[0][0]
            x = x + alpha * p
            r = r - alpha * Ap
            rsnew = r.T @ r
            p = r + (rsnew / rsold) * p
            rsold = rsnew
            times += 1
        return times, x

    def poly(self, X, W):
        """
        é€šè¿‡è‡ªå˜é‡Xå’Œå‚æ•°Wç”Ÿæˆç›¸åº”çš„å¤šé¡¹å¼å‡½æ•°ï¼Œæ–¹ä¾¿ç»˜å›¾æ¯”è¾ƒ
        param W:è‡ªå˜é‡å€¼
        return:å¯¹åº”çš„å¤šé¡¹å‡½æ•°å€¼
        """
        result = 0
        for i in range(self.m + 1):
            result += W[i][0] * np.power(X, i)
        return result



# start test
size = 10  # æ ·æœ¬æ•°æ®é‡
m = 5  # æ‹Ÿåˆå¤šé¡¹å¼å‡½æ•°çš„é˜¶æ•°
model = lab1(size, m)

# generate train data
x, y = model.generate_data()
# generate test data
x_test, y_test = model.generate_data()
# generate data of size 100
x_100, y_100 = lab1(size=100, m=5).generate_data()
# generate data of size 15
x_15, y_15 = lab1(size=15, m=5).generate_data()


# plot generate data and y = sin(2*pi*x) for comparison
plt.scatter(x, y)
x1 = np.linspace(0, 1, 1000)
plt.plot(x1, np.sin(2 * np.pi * x1))
plt.title('sample data')
plt.xlabel('x')
plt.ylabel('t')
plt.show()

# trans to generate X and T
X, T = model.trans(x, y)
# trans to generate X_Test and T_Test
X_Test, T_Test = model.trans(x_test, y_test)
# Analytical Solution
W_without_regulation = model.fit_without_regulation(X, T)
W_with_regulation = model.fit_with_regulation(X, T, 1e-7)

# é˜¶æ•°må–ä¸åŒå€¼æ—¶ï¼Œæœ€ä½³æ‹Ÿåˆæ›²çº¿æƒ…å†µ
Train_E_RMS = []
Test_E_RMS = []
for m in range(size):
    train = lab1(size, m)
    train_X, train_T = train.trans(x, y)
    test_X, test_T = train.trans(x_test, y_test)
    fit_without_regulation = model.fit_without_regulation(train_X, train_T)
    if m == 0 or m == 1 or m == 3 or m == 9:
        plt.scatter(x, y)
        plt.plot(x1, np.sin(2 * np.pi * x1))
        plt.plot(x1, train.poly(x1, fit_without_regulation))
        plt.title(f'fit with m = {m}')
        plt.xlabel('x')
        plt.ylabel('t')
        plt.show()
    train_error = model.error(train_X, fit_without_regulation, train_T)
    test_error = model.error(test_X, fit_without_regulation, test_T)
    Train_E_RMS.append(model.E_RMS(train_error))
    Test_E_RMS.append(model.E_RMS(test_error))

# plot E_RMS with M in training data and test data
M = np.linspace(0, size - 1, size)
plt.plot(M, Train_E_RMS, label='Training')
plt.plot(M, Test_E_RMS, label='Test')
plt.xlabel('M')
plt.ylabel('$E_{RMS}$')
plt.legend()
plt.show()

# compare fit with different size(15 and 100)
train = lab1(size=100, m=9)
plt.scatter(x_100, y_100)
plt.plot(x1, np.sin(2 * np.pi * x1))
train_X, train_T = train.trans(x_100, y_100)
fit_without_regulation = model.fit_without_regulation(train_X, train_T)
plt.plot(x1, train.poly(x1, fit_without_regulation))
plt.title('N=100')
plt.xlabel('x')
plt.ylabel('t')
plt.show()

train = lab1(size=15, m=9)
plt.scatter(x_15, y_15)
plt.plot(x1, np.sin(2 * np.pi * x1))
train_X, train_T = train.trans(x_15, y_15)
fit_without_regulation = model.fit_without_regulation(train_X, train_T)
plt.plot(x1, train.poly(x1, fit_without_regulation))
plt.title('N=15')
plt.xlabel('x')
plt.ylabel('t')
plt.show()

# åˆ†ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥å¯¹m=5,size=10æ—¶plotE_RMSéšlambdaçš„å˜åŒ–æƒ…å†µï¼Œç»“æœå‘ç°lambda=10^-7æ—¶æ•ˆæœæœ€å¥½
Train_E_RMS = []
Test_E_RMS = []
M = []
for log10_lambda in range(-10, 0):
    M.append(log10_lambda)
    Lambda = 10 ** log10_lambda
    train_error = model.error(X, model.fit_with_regulation(X, T, Lambda), T)
    test_error = model.error(X_Test, model.fit_with_regulation(X_Test, T_Test, Lambda), T_Test)
    Train_E_RMS.append(model.E_RMS(train_error))
    Test_E_RMS.append(model.E_RMS(test_error))
plt.plot(M, Train_E_RMS, label='Training')
plt.plot(M, Test_E_RMS, label='Test')
plt.xlabel('$log_{10}\lambda$')
plt.ylabel('$E_{RMS}$')
plt.legend()
plt.show()

# size=10,måˆ†åˆ«ç­‰äº0ï¼Œ5ï¼Œ9æ—¶çš„æ›²çº¿æ‹Ÿåˆå›¾(æœ‰æƒ©ç½šé¡¹ä¸æ— æƒ©ç½šé¡¹å¯¹æ¯”)
for size in [10]:
    for m in [0, 5, 9]:
        train = lab1(size, m)
        x, y = train.generate_data()
        train_X, train_T = train.trans(x, y)
        fit_without_regulation = train.fit_without_regulation(train_X, train_T)
        fit_with_regulation = train.fit_with_regulation(train_X, train_T)
        plt.scatter(x, y)
        plt.plot(x1, np.sin(2 * np.pi * x1))
        plt.plot(x1, train.poly(x1, fit_without_regulation))
        plt.title(f'fit without regulation size = {size} m = {m}')
        plt.xlabel('x')
        plt.ylabel('t')
        plt.show()
        plt.scatter(x, y)
        plt.plot(x1, np.sin(2 * np.pi * x1))
        plt.plot(x1, train.poly(x1, fit_with_regulation))
        plt.title(f'fit with regulation size = {size} m = {m}')
        plt.xlabel('x')
        plt.ylabel('t')
        plt.show()


# ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•å’Œå…±è½­æ¢¯åº¦æ³•è¾¾åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€æ¬¡æ•°çš„æ¯”è¾ƒ
times, W = model.gradient_descent(X, T, precision=1e-1)
print(f'ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-1æ‰€éœ€æ¬¡æ•°:{times}')
times, W = model.gradient_descent(X, T, precision=1e-1,Lambda=1e-7)
print(f'ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-1æ‰€éœ€æ¬¡æ•°:{times}')
times, W = model.conjugate_gradient(X, T, precision=1e-5)
print(f'ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•ï¼Œç²¾åº¦è¦æ±‚åœ¨1*10^-5æ‰€éœ€æ¬¡æ•°:{times}')

# ç›¸åŒè¿­ä»£æ¬¡æ•°ä¸‹ï¼Œæ¯”è¾ƒæ¢¯åº¦ä¸‹é™æ³•å’Œå…±è½­æ¢¯åº¦æ³•å¯¹æ›²çº¿çš„æ‹Ÿåˆæƒ…å†µ(è®¾å®šè¿­ä»£æ¬¡æ•°ä¸º100)
for size in [10, 30]:
    for m in [3, 9]:
        train = lab1(size, m)
        x, y = train.generate_data()
        train_X, train_T = train.trans(x, y)
        fit_gradient_descent = train.gradient_descent(train_X, train_T, times=100)
        fit_conjugate_gradient = train.conjugate_gradient(train_X, train_T, times=100)
        plt.scatter(x, y)
        plt.plot(x1, np.sin(2 * np.pi * x1))
        plt.plot(x1, train.poly(x1, fit_gradient_descent))
        plt.title(f'fit with gradient descent size = {size} m = {m}')
        plt.xlabel('x')
        plt.ylabel('t')
        plt.show()
        plt.scatter(x, y)
        plt.plot(x1, np.sin(2 * np.pi * x1))
        plt.plot(x1, train.poly(x1, fit_conjugate_gradient))
        plt.title(f'fit with conjugate gradient size = {size} m = {m}')
        plt.xlabel('x')
        plt.ylabel('t')
        plt.show()
```
